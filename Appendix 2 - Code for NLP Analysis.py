# -*- coding: utf-8 -*-
"""MASTER CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17CTEAabi7WtQTXA3007O3NvSw04EKXG8

## INSTALL AND LOAD PACKAGES
"""

!pip install emoji EmojiCloud scikit-learn fasttext

!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin

import json
import pandas as pd
import re
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
from wordcloud import WordCloud
import fasttext
import fasttext.util
from nltk.util import ngrams
import matplotlib.cm as cm
import regex as regex
import emoji
from EmojiCloud.plot import plot_dense_emoji_cloud
from EmojiCloud.emoji import EmojiManager
from EmojiCloud.canvas import RectangleCanvas
from EmojiCloud.vendors import APPLE
from IPython.display import Image
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import plotly.graph_objects as go
import plotly.offline as pyo

import pandas as pd
import re
import matplotlib.pyplot as plt
import nltk
import plotly.graph_objects as go
import plotly.offline as pyo
import matplotlib.cm as cm

"""## PREPROCESSING"""

# Merging JSON files

def jsonmerge(file_paths, group_name):
    all_data = pd.concat([
        pd.DataFrame([
            {'date': msg.get('date', ''),
             'message': ' '.join(part['text'] if isinstance(part, dict) else part for part in msg.get('text', '')) if isinstance(msg.get('text', ''), list) else msg.get('text', ''),
             'group': i + 1}
            for msg in json.load(open(file, 'r', encoding='utf-8'))['messages']
        ])
        for i, file in enumerate(file_paths)
    ], ignore_index=True)
    all_data.to_csv(f'combined_{group_name}.csv', index=False)
    return all_data

json_CS = [f'/content/drive/MyDrive/NLP data dissertation/CS - JSON/{i}.json' for i in range(1, 9)]
json_FR = [f'/content/drive/MyDrive/NLP data dissertation/FR - JSON/{i}.json' for i in range(1, 6)]
json_CG = [f'/content/drive/MyDrive/NLP data dissertation/CG - JSON/{i}.json' for i in range(1, 9)]

combined_CS = jsonmerge(json_CS, 'CS')
combined_FR = jsonmerge(json_FR, 'FR')
combined_CG = jsonmerge(json_CG, 'CG')

# Removing links, empty rows, trailing white spaces, and non-english-language data
# Converting to lowercase, and replacing mentions to maintain anonymity

model = fasttext.load_model('lid.176.bin')

def preprocess1(df):
    def prepro(text):
        text = re.sub(r'http\S+', '', text)
        text = re.sub(r'@\w+', '@redacted', text)
        text = text.replace('\n', ' ').lower().strip()
        if text in ['', 'nan', '@redacted'] or model.predict(text, k=1)[0][0] != '__label__en':
            return None
        return text

    df['message'] = df['message'].astype(str).apply(prepro)
    return df.dropna(subset=['message'])

combined_CS = preprocess1(pd.read_csv('combined_CS.csv'))
combined_FR = preprocess1(pd.read_csv('combined_FR.csv'))
combined_CG = preprocess1(pd.read_csv('combined_CG.csv'))

# Remove duplicates in new df

def removedups(df):
    return df.drop_duplicates(subset='message')

dataframes = {'CS': combined_CS, 'FR': combined_FR, 'CG': combined_CG}
for group_name, df in dataframes.items():
    df_nodup = removedups(df)
    df_nodup.to_csv(f'combined_{group_name}_nodup.csv', index=False)

combined_CS_nodup = pd.read_csv('combined_CS_nodup.csv')
combined_FR_nodup = pd.read_csv('combined_FR_nodup.csv')
combined_CG_nodup = pd.read_csv('combined_CG_nodup.csv')

dataframes = {
    'CS': (combined_CS, combined_CS_nodup),
    'FR': (combined_FR, combined_FR_nodup),
    'CG': (combined_CG, combined_CG_nodup)
}

for group_name, (df, df_nodup) in dataframes.items():
    rows = df.shape[0]
    rows_nodup = df_nodup.shape[0]
    print(f"rows in {group_name}: {rows}")
    print(f"rows in {group_name} without duplicates: {rows_nodup}")

saveto = '/content/drive/MyDrive/NLP data dissertation/'

combined_CS_nodup.to_csv(f'{saveto}combined_CS_nodup.csv', index=False)
combined_FR_nodup.to_csv(f'{saveto}combined_FR_nodup.csv', index=False)
combined_CG_nodup.to_csv(f'{saveto}combined_CG_nodup.csv', index=False)
combined_CS.to_csv(f'{saveto}combined_FR.csv', index=False)
combined_FR.to_csv(f'{saveto}combined_CG.csv', index=False)

"""## FREQUENCY ANALYSIS"""

# Tokenize, remove punctuation, stopwords, special characters, numbers (in new column)

nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))
specific_words_to_remove = ['im', 'ive', 'dont', 'ooh', 'youre', 'youll', 'la', 'oh', 'ill', "0o", "0s", "3a", "3b", "3d", "6b", "6o", "abst",
                  "ac", "accordance", "according", "accordingly", "across", "act", "actually", "ad", "added", "adj", "ae", "af", "affected",
                  "affecting", "affects", "after", "afterwards", "ag", "again", "against", "ah", "ain", "ain't", "aj", "al", "all", "allow",
                  "allows", "almost", "alone", "along", "already", "also", "although", "always", "am", "among", "amongst", "amoungst", "amount",
                  "an", "and", "announce", "another", "any", "anybody", "anyhow", "anymore", "anyone", "anything", "anyway", "anyways", "anywhere",
                  "ao", "ap", "apart", "apparently", "appear", "appreciate", "appropriate", "approximately", "ar", "are", "aren", "arent", "aren't",
                  "arise", "around", "as", "a's", "aside", "ask", "asking", "associated", "at", "au", "auth", "av", "available", "aw", "away", "awfully",
                  "ax", "ay", "az", "b", "b1", "b2", "b3", "ba", "back", "bc", "bd", "be", "became", "because", "become", "becomes", "becoming", "been", "before",
                  "beforehand", "behind", "below", "beside", "besides", "best", "better", "between",
                  "beyond", "bi", "bill", "biol", "bj", "bk", "bl", "bn", "both", "bottom", "bp", "br", "brief", "briefly", "bs", "bt", "bu", "but", "bx", "by", "c", "c1",
                  "c2", "c3", "ca", "call", "came", "can", "cannot", "cant", "can't", "cause", "causes", "cc", "cd", "ce", "certain", "certainly", "cf", "cg", "ch", "changes",
                  "ci", "cit", "cj", "cl", "clearly", "cm", "c'mon", "cn", "co", "com", "come", "comes", "con", "concerning", "consequently", "consider", "considering", "contain",
                  "containing", "contains", "corresponding", "could", "couldn", "couldnt", "couldn't", "course", "cp", "cq", "cr", "cry", "cs", "c's", "ct", "cu",
                  "currently", "cv", "cx", "cy", "cz", "d", "d2", "da", "date", "dc", "dd", "de", "describe", "described", "despite", "detail",
                  "df", "di", "did", "didn", "didn't", "different", "dj", "dk", "dl", "do", "does", "doesn", "doesn't", "doing", "don", "done", "don't", "down",
                  "downwards", "dp", "dr", "ds", "dt", "du", "due", "during", "dx", "dy", "e", "e2", "e3", "ea", "each", "ec", "ed", "edu", "ee", "ef",
                  "eg", "ei", "eight", "eighty", "either", "ej", "el", "eleven", "else", "elsewhere", "em", "empty", "en", "end", "ending", "enough", "entirely",
                  "eo", "ep", "eq", "er", "es", "especially", "est", "et", "et-al", "etc", "eu", "ev", "even", "ever", "every", "everybody", "everyone", "everything", "everywhere",
                  "ex", "exactly", "example", "except", "ey", "f", "f2", "fa", "far", "fc", "few", "ff", "fi", "fifteen", "fifth", "fify", "fill", "find", "fire",
                  "first", "five", "fix", "fj", "fl", "fn", "fo", "followed", "following", "follows", "for", "former", "formerly", "forth", "forty", "found", "four",
                  "fr", "from", "front", "fs", "ft", "fu", "full", "further", "furthermore", "fy", "g", "ga", "gave", "ge", "get", "gets", "getting", "gi", "give",
                  "given", "gives", "giving", "gj", "gl", "go", "goes", "going", "gone", "got", "gotten", "gr", "greetings", "gs", "gy", "h", "h2", "h3", "had", "hadn",
                  "hadn't", "happens", "hardly", "has", "hasn", "hasnt", "hasn't", "have", "haven", "haven't", "having", "he", "hed", "he'd", "he'll", "hello", "help",
                  "hence", "her", "here", "hereafter", "hereby", "herein", "heres", "here's", "hereupon", "hers", "herself", "hes", "he's", "hh", "hi", "hid", "him",
                  "himself", "his", "hither", "hj", "ho", "home", "hopefully", "how", "howbeit", "however", "how's", "hr", "hs", "http", "hu", "hundred", "hy", "i",
                  "i2", "i3", "i4", "i6", "i7", "i8", "ia", "ib", "ibid", "ic", "id", "i'd", "ie", "if", "ig", "ignored", "ih", "ii", "ij", "il", "i'll", "im", "i'm",
                  "immediate", "immediately", "in", "inasmuch", "inc", "indeed", "index", "indicate", "indicated",
                  "inner", "insofar", "instead", "interest", "into", "invention", "inward", "io", "ip", "iq", "ir", "is", "isn", "isn't", "it", "itd", "it'd", "it'll",
                  "its", "it's", "itself", "iv", "i've", "ix", "iy", "iz", "j", "jj", "jr", "js", "jt", "ju", "just", "k", "ke", "keep", "keeps", "kept", "kg", "kj", "km",
                  "known", "knows", "ko", "l", "l2", "la", "largely", "last", "lately", "later", "latter", "latterly", "lb", "lc", "le", "least", "les", "less", "lest", "let",
                  "lets", "let's", "lf", "like", "liked", "likely", "line", "little", "lj", "ll", "ll", "ln", "lo", "look", "looking", "looks", "los", "lr", "ls", "lt", "ltd",
                  "m", "m2", "ma", "made", "mainly", "make", "makes", "many", "may", "maybe", "me", "mean", "means", "meantime", "meanwhile", "merely", "mg", "might", "mightn",
                  "mightn't", "mill", "million", "mine", "miss", "ml", "mn", "mo", "more", "moreover", "most", "mostly", "move", "mr", "mrs", "ms", "mt", "mu", "much", "mug",
                  "must", "mustn", "mustn't", "my", "myself", "n", "n2", "na", "name", "namely", "nay", "nc", "nd", "ne", "near", "nearly", "necessarily", "necessary", "need",
                  "needn", "needn't", "needs", "neither", "never", "nevertheless", "new", "next", "ng", "ni", "nine", "ninety", "nj", "nl", "nn", "no", "nobody", "non", "none",
                  "nonetheless", "noone", "nor", "normally", "nos", "not", "noted", "nothing", "novel", "now", "nowhere", "nr", "ns", "nt", "ny", "o", "oa", "ob", "obtain",
                  "obtained", "obviously", "oc", "od", "of", "off", "often", "og", "oh", "oi", "oj", "ok", "okay", "ol", "old", "om", "omitted", "on", "once", "one", "ones",
                  "only", "onto", "oo", "op", "oq", "or", "ord", "os", "ot", "other", "others", "otherwise", "ou", "ought", "our", "ours", "ourselves", "out", "outside", "over",
                  "overall", "ow", "owing", "own", "ox", "oz", "p", "p1", "p2", "p3", "page", "pagecount", "pages", "par", "part", "particular", "particularly", "pas", "past",
                  "pc", "pd", "pe", "per", "perhaps", "pf", "ph", "pi", "pj", "pk", "pl", "placed", "please", "plus", "pm", "pn", "po", "poorly", "possible", "possibly",
                  "potentially", "pp", "pq", "pr", "predominantly", "present", "presumably", "previously", "primarily", "probably", "promptly", "proud", "provides", "ps", "pt",
                  "pu", "put", "py", "q", "qj", "qu", "que", "quickly", "quite", "qv", "r", "r2", "ra", "ran", "rather", "rc", "rd", "re", "readily", "really", "reasonably",
                  "recent", "recently", "ref", "refs", "regarding", "regardless", "regards", "related", "relatively", "research", "research-articl", "respectively", "resulted",
                  "resulting", "results", "rf", "rh", "ri", "right", "rj", "rl", "rm", "rn", "ro", "rq", "rr", "rs", "rt", "ru", "run", "rv", "ry", "s", "s2", "sa", "said",
                  "same", "saw", "say", "saying", "says", "sc", "sd", "se", "sec", "second", "secondly", "section", "see", "seeing", "seem", "seemed", "seeming", "seems",
                  "seen", "self", "selves", "sensible", "sent", "serious", "seriously", "seven", "several", "sf", "shall", "shan", "shan't", "she", "shed", "she'd", "she'll",
                  "shes", "she's", "should", "shouldn", "shouldn't", "should've", "show", "showed", "shown", "showns", "shows", "si", "side", "significant", "significantly",
                  "similar", "similarly", "since", "sincere", "six", "sixty", "sj", "sl", "slightly", "sm", "sn", "so", "some", "somebody", "somehow", "someone", "somethin",
                  "something", "sometime", "sometimes", "somewhat", "somewhere", "soon", "sorry", "sp", "specifically", "specified", "specify", "specifying", "sq", "sr", "ss",
                  "st", "still", "stop", "strongly", "sub", "substantially", "successfully", "such", "sufficiently", "suggest", "sup", "sure", "sy", "system", "sz", "t", "t1",
                  "t2", "t3", "take", "taken", "taking", "tb", "tc", "td", "te", "tell", "ten", "tends", "tf", "th", "than", "thank", "thanks", "thanx", "that", "that'll",
                  "thats", "that's", "that've", "the", "their", "theirs", "them", "themselves", "then", "thence", "there", "thereafter", "thereby", "thered", "therefore",
                  "therein", "there'll", "thereof", "therere", "theres", "there's", "thereto", "thereupon", "there've", "these", "they", "theyd", "they'd", "they'll", "theyre",
                  "they're", "they've", "thickv", "thin", "think", "third", "this", "thorough", "thoroughly", "those", "thou", "though", "thoughh", "thousand", "three",
                  "throug", "through", "throughout", "thru", "thus", "ti", "til", "tip", "tj", "tl", "tm", "tn", "to", "together", "too", "took", "top", "toward",
                  "tp", "tq", "tr", "tried", "tries", "truly", "try", "trying", "ts", "t's", "tt", "tv", "twelve", "twenty", "twice", "two", "tx", "u",
                  "u201d", "ue", "ui", "uj", "uk", "um", "un", "under", "unfortunately", "unless", "unlike", "until", "unto", "uo", "up", "upon", "ups",
                  "ur", "us", "use", "used", "useful", "usefully", "usefulness", "uses", "using", "usually", "ut", "v", "va", "value", "various", "vd", "ve", "ve",
                  "very", "via", "viz", "vj", "vo", "vol", "vols", "volumtype", "vq", "vs", "vt", "vu", "w", "wa", "want", "wants", "was", "wasn", "wasnt", "wasn't",
                  "way", "we", "wed", "we'd", "well", "we'll", "well-b", "went", "were", "we're", "weren", "werent", "weren't", "we've", "what",
                  "whatever", "what'll", "whats", "what's", "when", "whence", "whenever", "when's", "where", "whereafter", "whereas", "whereby", "wherein", "wheres",
                  "where's", "whereupon", "wherever", "whether", "which", "while", "whim", "whither", "who", "whod", "whoever", "who'll", "whom", "whomever",
                  "whos", "who's", "whose", "why", "why's", "wi", "widely", "will", "willing", "wish", "with", "without", "wo", "wont", "won't",
                  "words", "would", "wouldn", "wouldnt", "wouldn't", "www", "x", "x1", "x2", "x3", "xf", "xi", "xj", "xk", "xl", "xn", "xo", "xs", "xt", "xv", "xx",
                  "y", "y2", "yes", "yet", "yj", "yl", "you", "youd", "you'd", "you'll", "your", "youre", "you're", "yours", "you've", "yr",
                  "ys", "yt", "z", "zero", "zi", "zz", "redacted", "didnt", "lot", "isnt", "doesnt", "thing", "redacteds", "itits", "redactedter", "people"
]

def preprocess2(text):
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word.lower() not in stop_words]
    tokens = [word for word in tokens if word.lower() not in specific_words_to_remove]
    return tokens

combined_CS['tokens'] = combined_CS['message'].apply(preprocess2)
combined_FR['tokens'] = combined_FR['message'].apply(preprocess2)

def findmostfreq(list, total, top_n=100):
    alltokens = [token for tokens in list for token in tokens if token != 'nan']
    counter = Counter(alltokens)
    mostcommonw = counter.most_common(top_n)
    return [(word, freq, (freq / total) * 100) for word, freq in mostcommonw]

def wordfreqsave(df, filename):
    total_tokens = sum(len(tokens) for tokens in df['tokens'])
    mostfreqw = findmostfreq(df['tokens'], total_tokens)
    mostfreqw_df = pd.DataFrame(mostfreqw, columns=['word', 'frequency', 'percentage'])
    mostfreqw_df.to_csv(f'{saveto}{filename}', index=False)
    return mostfreqw_df

mostfreqwCS_df = wordfreqsave(combined_CS, 'mostfreqwCS.csv')
mostfreqwFR_df = wordfreqsave(combined_FR, 'mostfreqwFR.csv')

def make_wordcloud(df, title):
    word_freq_dict = dict(zip(df['word'], df['frequency']))
    wordcloud = WordCloud(width=600, height=400, background_color='white', colormap='twilight_shifted').generate_from_frequencies(word_freq_dict)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, size=15)
    plt.axis('off')
    plt.show()

make_wordcloud(mostfreqwCS_df, 'Top 100 Most Frequent Words Used by CS Groups')
make_wordcloud(mostfreqwFR_df, 'Top 100 Most Frequent Words Used by FR Groups')

# Bigrams

ignore_terms = {'awakenedspecies', 'httpstmedrue', 'davidavocadowolfe', 'david', 'acocado', 'wolfe', 'wwwcultivateelevatecom', 'cultivateelevatecom', 'redacted'}

def findmostfreq_bigrams(tokens_list, total, top_n=100):
    allbigrams = [bigram for tokens in tokens_list for bigram in ngrams(tokens, 2)]
    filtered = [bigram for bigram in allbigrams if not any(term in ignore_terms for term in bigram)]
    counter = Counter(filtered)
    mostcommonb = counter.most_common(top_n)
    return [(" ".join(bigram), freq, (freq / total) * 100) for bigram, freq in mostcommonb]

def bigramfreqsave(df, filename):
    total = sum(len(tokens) for tokens in df['tokens'])
    mostfreqb = findmostfreq_bigrams(df['tokens'], total)
    mostfreqb_df = pd.DataFrame(mostfreqb, columns=['bigram', 'frequency', 'percentage'])
    mostfreqb_df.to_csv(f'{saveto}{filename}', index=False)
    return mostfreqb_df

mostfreqbCS_df = bigramfreqsave(combined_CS, 'mostfreqbCS.csv')
mostfreqbFR_df = bigramfreqsave(combined_FR, 'mostfreqbFR.csv')

def joinb(df):
    df['bigram'] = df['bigram'].apply(lambda x: '_'.join(x.split()) if ' ' in x else x)
    return df

mostfreqbCS_df = joinb(mostfreqbCS_df)
mostfreqbFR_df = joinb(mostfreqbFR_df)

def makewordcloud(df, title):
    word_freq_dict = dict(zip(df['bigram'], df['frequency']))
    wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='twilight_shifted').generate_from_frequencies(word_freq_dict)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(title, size=20)
    plt.axis('off')
    plt.show()

makewordcloud(mostfreqbCS_df, 'Top 100 Most Frequent Bigrams Used by CS Groups')
makewordcloud(mostfreqbFR_df, 'Top 100 Most Frequent Bigrams Used by FR Groups')

def ext_emojis(txt):
    emjs = [e['emoji'] for e in emoji.emoji_list(txt)]
    return emjs

def full_unicode(emj):
    return ' '.join(f'U+{ord(char):04X}' for char in emj)

def emoji_freq_df(df):
    emj_counter = Counter()
    for msg in df['message']:
        emjs = ext_emojis(msg)
        for emj in emjs:
            full_unicode_str = full_unicode(emj)
            emj_counter[full_unicode_str] += 1

    emj_df = pd.DataFrame(emj_counter.items(), columns=['Unicode', 'Freq'])
    emj_df['Emoji'] = emj_df['Unicode'].apply(lambda x: ''.join(chr(int(char[2:], 16)) for char in x.split()))
    emj_df['Pct'] = (emj_df['Freq'] / emj_df['Freq'].sum()) * 100
    emj_df = emj_df.sort_values(by='Freq', ascending=False)
    return emj_df

emoji_df_CS2 = emoji_freq_df(combined_CS)
emoji_df_FR2 = emoji_freq_df(combined_FR)

def emoji_weight_dict(df, top_n=50):
    top_df = df.head(top_n)
    emj_weight_dict = {unicode_seq.replace('U+', '').lower(): freq for unicode_seq, freq in zip(top_df['Unicode'], top_df['Freq'])}
    return emj_weight_dict

emj_weight_dict_CS = emoji_weight_dict(emoji_df_CS2, top_n=50)
emj_weight_dict_FR = emoji_weight_dict(emoji_df_FR2, top_n=50)
emj_list_CS = EmojiManager.create_list_from_single_vendor(emj_weight_dict_CS, APPLE)
emj_list_FR = EmojiManager.create_list_from_single_vendor(emj_weight_dict_FR, APPLE)
canvas_CS = RectangleCanvas(72*10, 72*4)
canvas_FR = RectangleCanvas(72*10, 72*4)
im_CS = plot_dense_emoji_cloud(canvas_CS, emj_list_CS)
im_CS.save('emoji_cloud_CS.png')
im_FR = plot_dense_emoji_cloud(canvas_FR, emj_list_FR)
im_FR.save('emoji_cloud_FR.png')

display(Image('emoji_cloud_CS.png'))
display(Image('emoji_cloud_FR.png'))

"""## TOPIC MODELLING"""

df1 = pd.read_csv('/content/drive/MyDrive/NLP data dissertation/combined_FR_nodup.csv')
df2 = pd.read_csv('/content/drive/MyDrive/NLP data dissertation/combined_CS_nodup.csv')
timestamps1 = df1.date.to_list()
timestamps2 = df2.date.to_list()
df1['message'] = df1['message'].fillna('')
df2['message'] = df2['message'].fillna('')

vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
dtm = vectorizer.fit_transform(df1['message'])

num_topics = 20
lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
lda.fit(dtm)

topic_assignments = lda.transform(dtm)
document_topics = topic_assignments.argmax(axis=1)
df1['Topic'] = document_topics

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

no_top_words = 30
tf_feature_names = vectorizer.get_feature_names_out()
display_topics(lda, tf_feature_names, no_top_words)

newcats = {
    0: "Miscellanious discussion",
    1: "Deep state conspiracy and censorship",
    2: "Anti-Semitism",
    3: "Deep state conspiracy and censorship",
    4: "Deep state conspiracy and censorship",
    5: "Trump and patriotism",
    6: "Health conspiracies",
    7: "Deep state conspiracy and censorship",
    8: "Current events",
    9: "Conflict and war",
    10: "Religious beliefs and faith",
    11: "Child trafficking",
    12: "Trump and patriotism",
    13: "Political rallying",
    14: "Political corruption",
    15: "COVID-19 anti-vaccine rhetoric and conspiracy",
    16: "Miscellaneous conspiracies",
    17: "Deep state conspiracy and censorship",
    18: "Tech conspiracies",
    19: "Trump and patriotism"
}

df1['Category'] = df1['Topic'].map(newcats)
file_path = '/content/drive/MyDrive/NLP data dissertation/finalFR.csv'
df1.to_csv(file_path, index=False)

vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
dtm = vectorizer.fit_transform(df2['message'])

num_topics = 20
lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
lda.fit(dtm)

topic_assignments = lda.transform(dtm)
document_topics = topic_assignments.argmax(axis=1)
df2['Topic'] = document_topics

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

no_top_words = 30
tf_feature_names = vectorizer.get_feature_names_out()
display_topics(lda, tf_feature_names, no_top_words)

newcats2 = {
    0: "Health concerns",
    1: "Astrology and pseudoscience",
    2: "Miscellaneous discussion",
    3: "Political and government conspiracies",
    4: "Spirituality, healing, and esoteric beliefs",
    5: "Health concerns",
    6: "Astrology and pseudoscience",
    7: "COVID-19 anti-vaccine rhetoric and conspiracy",
    8: "MSM and misinformation",
    9: "Prayer and worship",
    10: "Gratitude and appreciation",
    11: "Spirituality, healing, and esoteric beliefs",
    12: "COVID-19 vaccine concerns and conspiracies",
    13: "Gardening and DIY",
    14: "Spirituality, healing, and esoteric beliefs",
    15: "Political and government conspiracies",
    16: "Miscellaneous discussion",
    17: "Nutrition and alternative health practices",
    18: "Nutrition and alternative health practices",
    19: "Nutrition and alternative health practices"
}

df2['Category'] = df2['Topic'].map(newcats2)
file_path = '/content/drive/MyDrive/NLP data dissertation/finalCS.csv'
df2.to_csv(file_path, index=False)

file_path = '/content/drive/MyDrive/NLP data dissertation/finalCS.csv'
df2 = pd.read_csv(file_path)
df2['Category'] = df2['Category'].replace('COVID-19 vaccine concerns and conspiracies', 'COVID-19 anti-vaccine rhetoric and conspiracy')
df2.to_csv(file_path, index=False)
df1 = pd.read_csv('/content/drive/MyDrive/NLP data dissertation/finalFR.csv')

color_seq = ['#7FFFD4', '#458B74', '#E3CF57', '#FF4040', '#FF7F50', '#FF69B4', '#FFB90F', '#AB82FF',
             '#bcbd22', '#17becf', '#1e8449', '#FFE1FF', '#3498db', '#698B22', '#9b59b6', '#95a5a6',
             '#FF6347', '#4682B4']

df1['date'] = pd.to_datetime(df1['date'])
df2['date'] = pd.to_datetime(df2['date'])
df1_pivot = df1.groupby([df1['date'].dt.date, 'Category']).size().unstack(fill_value=0)
df2_pivot = df2.groupby([df2['date'].dt.date, 'Category']).size().unstack(fill_value=0)

def plot_topic_freq(df, title, filename):
    total_msgs_per_day = df.sum(axis=1)
    df_pct = df.div(total_msgs_per_day, axis=0) * 100
    fig = go.Figure()

    for i, (name, series) in enumerate(df_pct.items()):
        series.index = pd.to_datetime(series.index)
        series_idx_str = series.index.strftime('%Y-%m-%d').tolist()
        fig.add_trace(go.Scatter(
            x=series_idx_str,
            y=series.values,
            mode='lines',
            name=name,
            line=dict(color=color_seq[i % len(color_seq)], width=1)
        ))

    fig.update_layout(
        title=title,
        xaxis_title='Time',
        yaxis_title='Percentage of Messages',
        legend_title='Topics',
        xaxis=dict(range=['2021-01-01', '2024-05-01'])
    )

    pyo.plot(fig, filename=filename + '.html')
    fig.show()
    fig.write_html(filename + ".html")

plot_topic_freq(df1_pivot, 'Relative Topic Frequency Over Time for FR Groups', 'topic_freq_df1')
plot_topic_freq(df2_pivot, 'Relative Topic Frequency Over Time for CS Groups', 'topic_freq_df2')

def calc_topic_dist(df):
    topic_counts = df['Category'].value_counts(normalize=True).reset_index()
    topic_counts.columns = ['Category', 'Percentage']
    topic_counts['Percentage'] *= 100
    return topic_counts

def plot_pie_chart(topic_counts, title, filename):
    fig = go.Figure(data=[go.Pie(labels=topic_counts['Category'], values=topic_counts['Percentage'])])
    fig.update_layout(title_text=title)
    fig.show()
    fig.write_html(filename + ".html")

topic_counts_df1 = calc_topic_dist(df1)
plot_pie_chart(topic_counts_df1, 'Topic Distribution for FR Groups', 'topic_dist_FR')
topic_counts_df2 = calc_topic_dist(df2)
plot_pie_chart(topic_counts_df2, 'Topic Distribution in CS Groups', 'topic_dist_CS')

"""## SPACY EMBEDDINGS

### Packages
"""

!python -m spacy download en_core_web_sm

!pip install spacy-transformers transformers

import pandas as pd
import spacy
import en_core_web_sm
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
import textwrap

"""### Embeddings"""

FRlist = [
    "god bless trump",
    "money is a lie, history is a lie, globalism is a fraud. when will people finally wakeup and realize they have been had?",
    "all these evil people need to be removed permanently!",
    "thank you!",
    "fuck that! mandating/forcing vaccination is a nuremberg code violation!",
    "it's been time for a change for years"
]

CSlist = [
    "this guy is a maniac. and the entire establishment that supports him is corrupt to the core!!",
    "president trump sent by god  he is the chosen one",
    "we’ve all got to fight for freedom from forced vaccines",
    "who proposals could strip nations of their sovereignty, create worldwide totalitarian state, expert warns",
    "thank you!",
    "incredibly beautiful, thank you....love you brother"
]

CGlist = [
    "what kind of marketing is being done right now? i'm just curious",
    "thank you!",
    "you need to buy within 7 days of booking your trip to be eligible to claim under cancellation for any reason though",
    "I still feel very bad we lost easy two points thanks to him.",
    "it is a beautiful city",
    "so filling out paperwork guarantees you the tokens?",
]

nlp = en_core_web_sm.load()

def make_spacy_embs(text_list):
    return np.array([nlp(text).vector for text in text_list])

spacy_embs_FR = make_spacy_embs(FRlist)
spacy_embs_CS = make_spacy_embs(CSlist)
spacy_embs_CG = make_spacy_embs(CGlist)

spacy_similarity_matrix = cosine_similarity(spacy_embs_FR, spacy_embs_CS)
spacy_normalised_sim_matrix = (spacy_similarity_matrix + 1) / 2
spacy_average_similarity = spacy_normalised_sim_matrix.mean()
print("Average pairwise cosine similarity FR_CS with Spacy:", spacy_average_similarity)

def wrap_labels(labels, width=30):
    return [textwrap.fill(label, width) for label in labels]

wrapped_FRlist = wrap_labels(FRlist)
wrapped_CSlist = wrap_labels(CSlist)

plt.figure(figsize=(8, 6))
ax = sns.heatmap(spacy_normalised_sim_matrix, annot=True, fmt=".2f", cmap='viridis', cbar_kws={'label': 'Similarity'},
                 xticklabels=wrapped_CSlist, yticklabels=wrapped_FRlist, vmin=0, vmax=1)

plt.title('Heatmap of Pairwise Cosine Similarity for FR_CS with Spacy embeddings')
plt.xlabel('CS message')
plt.ylabel('FR message')
plt.show()

"""## SBERT EMBEDDINGS

### Packages
"""

!pip install -U sentence-transformers

!pip install datasets

from sentence_transformers import SentenceTransformer
import pandas as pd
from transformers import BertTokenizer, BertModel
import torch

"""### Embeddings"""

model = SentenceTransformer("all-mpnet-base-v2")

FR_embeddings = model.encode(FRlist)
CS_embeddings = model.encode(CSlist)

similarities = cosine_similarity(FR_embeddings, CS_embeddings)
s_normalised_sim_matrix = (similarities + 1) / 2

def wrap_labels(labels, width=30):
    return [textwrap.fill(label, width) for label in labels]

wrapped_FRlist = wrap_labels(FRlist)
wrapped_CSlist = wrap_labels(CSlist)

plt.figure(figsize=(8, 6))
ax = sns.heatmap(s_normalised_sim_matrix, annot=True, fmt=".2f", cmap='viridis', cbar_kws={'label': 'Similarity'},
                 xticklabels=wrapped_CSlist, yticklabels=wrapped_FRlist, vmin=0, vmax=1)

plt.title('Heatmap of Pairwise Cosine Similarity for FR and CS with SBERT')
plt.xlabel('CS message')
plt.ylabel('FR message')
plt.show()

"""## FINE-TUNED SBERT

### Fine-tuning

code source: https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/sts/training_stsbenchmark_continue_training.py
"""

import logging
import sys
import traceback
from datetime import datetime
from transformers import AutoModel, AutoTokenizer
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers.similarity_functions import SimilarityFunction
from sentence_transformers.trainer import SentenceTransformerTrainer
from sentence_transformers.training_args import SentenceTransformerTrainingArguments

logging.basicConfig(format="%(asctime)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S", level=logging.INFO)

model_name = "sentence-transformers/all-mpnet-base-v2"
train_batch_size = 16
num_epochs = 4
output_dir = (
    "output/training_stsbenchmark_" + model_name.replace("/", "-") + "-" + datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
)

model = SentenceTransformer(model_name)

train_dataset = load_dataset("sentence-transformers/stsb", split="train")
eval_dataset = load_dataset("sentence-transformers/stsb", split="validation")
test_dataset = load_dataset("sentence-transformers/stsb", split="test")
logging.info(train_dataset)

train_loss = losses.CosineSimilarityLoss(model=model)

dev_evaluator = EmbeddingSimilarityEvaluator(
    sentences1=eval_dataset["sentence1"],
    sentences2=eval_dataset["sentence2"],
    scores=eval_dataset["score"],
    main_similarity=SimilarityFunction.COSINE,
    name="sts-dev",
)

args = SentenceTransformerTrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_epochs,
    per_device_train_batch_size=train_batch_size,
    per_device_eval_batch_size=train_batch_size,
    warmup_ratio=0.1,
    fp16=True,
    bf16=False,
    eval_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    logging_steps=100,
    run_name="sts",
)

trainer = SentenceTransformerTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    loss=train_loss,
    evaluator=dev_evaluator,
)
trainer.train()

test_evaluator = EmbeddingSimilarityEvaluator(
    sentences1=test_dataset["sentence1"],
    sentences2=test_dataset["sentence2"],
    scores=test_dataset["score"],
    main_similarity=SimilarityFunction.COSINE,
    name="sts-test",
)
test_evaluator(model)

final_output_dir = f"{output_dir}/final"
model.save(final_output_dir)

trainedmodel1 = SentenceTransformer(final_output_dir)
trainedmodel1.save('/content/drive/MyDrive/NLP data dissertation/trained_model1')

"""### Results"""

trainedmodel1 = SentenceTransformer('/content/drive/MyDrive/NLP data dissertation/trained_model1')

membeddings_1 = trainedmodel1.encode(FRlist, convert_to_tensor=True)
membeddings_2 = trainedmodel1.encode(CSlist, convert_to_tensor=True)
membeddings_3 = trainedmodel1.encode(CGlist, convert_to_tensor=True)
membeddings_1 = membeddings_1.cpu().numpy()
membeddings_2 = membeddings_2.cpu().numpy()
membeddings_3 = membeddings_3.cpu().numpy()

from sklearn.metrics.pairwise import cosine_similarity
t4similarity_matrix = cosine_similarity(membeddings_1, membeddings_2)
t5similarity_matrix = cosine_similarity(membeddings_1, membeddings_3)
t6similarity_matrix = cosine_similarity(membeddings_2, membeddings_3)
t4normalized_sim_matrix = (t4similarity_matrix + 1) / 2
t5normalized_sim_matrix = (t5similarity_matrix + 1) / 2
t6normalized_sim_matrix = (t6similarity_matrix + 1) / 2
t4average_similarity = t4normalized_sim_matrix.mean()
t5average_similarity = t5normalized_sim_matrix.mean()
t6average_similarity = t6normalized_sim_matrix.mean()
print("Average pairwise cosine similarity:", t4average_similarity)
print("Average pairwise cosine similarity:", t5average_similarity)
print("Average pairwise cosine similarity:", t6average_similarity)

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import textwrap

def wrap_labels(labels, width=30):
    return [textwrap.fill(label, width) for label in labels]

wrapped_list1 = wrap_labels(FRlist)
wrapped_list2 = wrap_labels(CSlist)

plt.figure(figsize=(8, 6))
ax = sns.heatmap(t4normalized_sim_matrix, annot=True, fmt=".2f", cmap='viridis', cbar_kws={'label': 'Similarity'},
                 xticklabels=wrapped_list2, yticklabels=wrapped_list1, vmin=0, vmax=1)

plt.title('Heatmap of Pairwise Cosine Similarity for FR_CS with fine-tuned SBERT embeddings')
plt.xlabel('CSlist')
plt.ylabel('FRlist')
plt.show()

"""### Full analysis"""

import pandas as pd
import re
import torch
from sentence_transformers import SentenceTransformer
import seaborn as sns
import matplotlib.pyplot as plt

def remove_emojis(text):
    if isinstance(text, str):
        emoji_pattern = re.compile(
            "["
            u"\U0001F600-\U0001F64F"
            u"\U0001F300-\U0001F5FF"
            u"\U0001F680-\U0001F6FF"
            u"\U0001F700-\U0001F77F"
            u"\U0001F780-\U0001F7FF"
            u"\U0001F800-\U0001F8FF"
            u"\U0001F900-\U0001F9FF"
            u"\U0001FA00-\U0001FA6F"
            u"\U0001FA70-\U0001FAFF"
            u"\U00002702-\U000027B0"
            u"\U000024C2-\U0001F251"
            "]+", flags=re.UNICODE)
        return emoji_pattern.sub(r'', text)
    else:
        return text

dfCS = pd.read_csv('/content/drive/MyDrive/NLP data dissertation/finalCS.csv')
dfFR = pd.read_csv('/content/drive/MyDrive/NLP data dissertation/finalFR.csv')
dfCG = pd.read_csv('/content/drive/MyDrive/NLP data dissertation/combined_CG_nodup.csv')
dfCS['message'] = dfCS['message'].apply(remove_emojis)
dfFR['message'] = dfFR['message'].apply(remove_emojis)
dfCG['message'] = dfCG['message'].apply(remove_emojis)
dfCS['message'] = dfCS['message'].str.strip()
dfFR['message'] = dfFR['message'].str.strip()
dfCG['message'] = dfCG['message'].str.strip()

dfCS.drop_duplicates(subset=['message'], inplace=True)
dfFR.drop_duplicates(subset=['message'], inplace=True)
dfCG.drop_duplicates(subset=['message'], inplace=True)

dfCS_sampled = dfCS.sample(n=25000)
dfFR_sampled = dfFR.sample(n=25000)
dfCG_sampled = dfCG.sample(n=25000)
corpusCS = dfCS_sampled['message'].tolist()
corpusFR = dfFR_sampled['message'].tolist()
corpusCG = dfCG_sampled['message'].tolist()

trainedmodel1 = SentenceTransformer('/content/drive/MyDrive/NLP data dissertation/trained_model1')

batch_size = 500
embeddingsFR = []
embeddingsCS = []
embeddingsCG = []

for i in range(0, len(corpusFR), batch_size):
    batch_texts = corpusFR[i:i+batch_size]
    batch_embeddings = trainedmodel1.encode(batch_texts, convert_to_tensor=True)
    embeddingsFR.append(batch_embeddings)

for i in range(0, len(corpusCS), batch_size):
    batch_texts = corpusCS[i:i+batch_size]
    batch_embeddings = trainedmodel1.encode(batch_texts, convert_to_tensor=True)
    embeddingsCS.append(batch_embeddings)

for i in range(0, len(corpusCG), batch_size):
    batch_texts = corpusCG[i:i+batch_size]
    batch_embeddings = trainedmodel1.encode(batch_texts, convert_to_tensor=True)
    embeddingsCG.append(batch_embeddings)

embeddingsFR = torch.cat(embeddingsFR)
embeddingsCS = torch.cat(embeddingsCS)
embeddingsCG = torch.cat(embeddingsCG)

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

def get_cosine_sim(embeddingsFR, embeddingsCS, batch_size=500):
    n = embeddingsFR.shape[0]
    m = embeddingsCS.shape[0]
    scores = np.zeros((n, m))

    for i in range(0, n, batch_size):
        for j in range(0, m, batch_size):
            end_i = min(i + batch_size, n)
            end_j = min(j + batch_size, m)

            batch1 = embeddingsFR[i:end_i]
            batch2 = embeddingsCS[j:end_j]

            with torch.no_grad():
                batch_scores = torch.nn.functional.cosine_similarity(
                    batch1.unsqueeze(1),
                    batch2.unsqueeze(0),
                    dim=-1
                ).cpu().numpy()

            scores[i:end_i, j:end_j] = batch_scores

    return scores

cosine_scores_FR_CS = get_cosine_sim(embeddingsFR, embeddingsCS, batch_size=500)

cosine_scores_FR_CG = get_cosine_sim(embeddingsFR, embeddingsCG, batch_size=500)

cosine_scores_CS_CG = get_cosine_sim(embeddingsCS, embeddingsCG, batch_size=500)

def get_norm_scores(scores, batch_size=500):
    n, m = scores.shape
    normalized_scores = np.zeros((n, m))

    for i in range(0, n, batch_size):
        for j in range(0, m, batch_size):
            end_i = min(i + batch_size, n)
            end_j = min(j + batch_size, m)

            batch_scores = scores[i:end_i, j:end_j]
            normalized_batch_scores = (batch_scores + 1) / 2
            normalized_scores[i:end_i, j:end_j] = normalized_batch_scores

    return normalized_scores

normalized_scoresCSFR = get_norm_scores(cosine_scores_FR_CS, batch_size=500)

normalized_scoresFRCG = get_norm_scores(cosine_scores_FR_CG, batch_size=500)

normalized_scoresCSCG = get_norm_scores(cosine_scores_CS_CG, batch_size=500)

similarity_values = normalized_scoresCSFR.flatten()
quartiles = np.percentile(similarity_values, [25, 50, 75])
num_bins = 50
colormap = plt.get_cmap('viridis')
plt.figure(figsize=(10, 6))
n, bins, patches = plt.hist(similarity_values, bins=num_bins, range=(0, 1), alpha=0.7)

for i in range(num_bins):
    patches[i].set_facecolor(colormap(i / num_bins))

for quartile in quartiles:
    plt.axvline(quartile, color='red', linestyle='dashed', linewidth=1)

box_text = f"Q1 = {quartiles[0]:.4f}\nQ2 = {quartiles[1]:.4f}\nQ3 = {quartiles[2]:.4f}"
plt.gca().text(0.95, 0.95, box_text, fontsize=12, verticalalignment='top', horizontalalignment='right',
               bbox=dict(facecolor='white', alpha=0.5), transform=plt.gca().transAxes)

plt.title('Spread of Pairwise Normalized Cosine Similarities FR_CT for Sample 1')
plt.xlabel('Normalized Cosine Similarity')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

print(f"Quartile values: {quartiles}")

plt.figure(figsize=(10, 8))
sns.heatmap(normalized_scoresCSFR[:50, :50], cmap='viridis', annot=False, vmin=0, vmax=1)
plt.title('Heatmap of Pairwise Similarity Scores FR_CT (subset)')
plt.xlabel('corpusCT')
plt.ylabel('corpusFR')
plt.show()

threshold = 0.85
high_similarity_pairs = np.argwhere(cosine_scores > threshold)

print(f"Sentence pairs with similarity above {threshold}:")

for idx in high_similarity_pairs:
    cs_idx, fr_idx = idx
    cs_sentence = corpusFR[cs_idx]
    fr_sentence = corpusCG[fr_idx]
    similarity_score = cosine_scores[cs_idx, fr_idx]
    print(f"FR Sentence: {cs_sentence}")
    print(f"CT Sentence: {fr_sentence}")
    print(f"Similarity Score: {similarity_score}\n")

"""## ANOVA + Tukey HSD"""

import numpy as np

flattened_scoresCSFR = normalized_scoresCSFR.flatten()
flattened_scoresFRCG = normalized_scoresFRCG.flatten()
flattened_scoresCSCG = normalized_scoresCSCG.flatten()

np.save('flattened_scoresCSFR.npy', flattened_scoresCSFR)
np.save('flattened_scoresFRCG.npy', flattened_scoresFRCG)
np.save('flattened_scoresCSCG.npy', flattened_scoresCSCG)

import numpy as np
from scipy.stats import f_oneway

flattened_scoresCSFR = np.load('flattened_scoresCSFR.npy')
flattened_scoresFRCG = np.load('flattened_scoresFRCG.npy')
flattened_scoresCSCG = np.load('flattened_scoresCSCG.npy')

anova_result = f_oneway(flattened_scoresCSFR, flattened_scoresFRCG, flattened_scoresCSCG)

print(f"ANOVA F-statistic: {anova_result.statistic}")
print(f"ANOVA p-value: {anova_result.pvalue}")

import numpy as np

flattened_scoresCSFR = np.load('flattened_scoresCSFR.npy')
flattened_scoresFRCG = np.load('flattened_scoresFRCG.npy')
flattened_scoresCSCG = np.load('flattened_scoresCSCG.npy')

sample_fraction = 0.1

downsampled_scoresCSFR = np.random.choice(flattened_scoresCSFR, int(len(flattened_scoresCSFR) * sample_fraction), replace=False)
downsampled_scoresFRCG = np.random.choice(flattened_scoresFRCG, int(len(flattened_scoresFRCG) * sample_fraction), replace=False)
downsampled_scoresCSCG = np.random.choice(flattened_scoresCSCG, int(len(flattened_scoresCSCG) * sample_fraction), replace=False)

scores = np.concatenate([downsampled_scoresCSFR, downsampled_scoresFRCG, downsampled_scoresCSCG])
groups = ['FR_CS'] * len(downsampled_scoresCSFR) + ['FR_CG'] * len(downsampled_scoresFRCG) + ['CS_CG'] * len(downsampled_scoresCSCG)

import pandas as pd
from statsmodels.stats.multicomp import pairwise_tukeyhsd

tukey_result = pairwise_tukeyhsd(scores, groups, alpha=0.05)
print(tukey_result)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

flattened_scoresCSFR = np.load('flattened_scoresCSFR.npy')
flattened_scoresFRCG = np.load('flattened_scoresFRCG.npy')
flattened_scoresCSCG = np.load('flattened_scoresCSCG.npy')

subset_size = 10000

subset_CSFR = np.random.choice(flattened_scoresCSFR, subset_size, replace=False)
subset_FRCG = np.random.choice(flattened_scoresFRCG, subset_size, replace=False)
subset_CSCG = np.random.choice(flattened_scoresCSCG, subset_size, replace=False)

data = {
    'FR_CS': subset_CSFR,
    'FR_CG': subset_FRCG,
    'CS_CG': subset_CSCG
}

df = pd.DataFrame(data)

plt.figure(figsize=(10, 6))
df.boxplot()
plt.title('Cosine Similarity Distributions (Subset)')
plt.ylabel('Cosine Similarity')
plt.show()

import numpy as np
flattened_scoresCSFR = np.load('flattened_scoresCSFR.npy')
flattened_scoresFRCG = np.load('flattened_scoresFRCG.npy')
flattened_scoresCSCG = np.load('flattened_scoresCSCG.npy')
variance_CS_CG = np.var(flattened_scoresCSCG)
variance_FR_CG = np.var(flattened_scoresFRCG)
variance_FR_CS = np.var(flattened_scoresCSFR)

print(f"Variance of CS_CG: {variance_CS_CG}")
print(f"Variance of FR_CG: {variance_FR_CG}")
print(f"Variance of FR_CS: {variance_FR_CS}")

mean_CS_CG = np.mean(flattened_scoresCSCG)
mean_FR_CG = np.mean(flattened_scoresFRCG)
mean_FR_CS = np.mean(flattened_scoresCSFR)

print(f"Mean of CS_CG: {mean_CS_CG}")
print(f"Mean of FR_CG: {mean_FR_CG}")
print(f"Mean of FR_CS: {mean_FR_CS}")